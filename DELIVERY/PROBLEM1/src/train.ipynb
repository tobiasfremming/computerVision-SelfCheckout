{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, InputLayer, Dropout\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms, models\n",
    "from tqdm import tqdm  # Import tqdm for progress bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "TRAIN_DIR = 'images/NGD_HACK_TRAIN_CROPPED'\n",
    "TEST_DIR = 'images/NGD_HACK_VALIDATION_CROPPED'\n",
    "IMG_SIZE = (224, 224)\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10\n",
    "SEED = 123\n",
    "NUM_CLASSES = 26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "\n",
    "train_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "test_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    TRAIN_DIR,\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False  # Turn off shuffle for consistent label ordering\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    TEST_DIR,\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "num_train_samples = train_generator.samples\n",
    "num_test_samples = test_generator.samples\n",
    "num_classes = train_generator.num_classes\n",
    "\n",
    "print(f\"Train samples: {num_train_samples}, Test samples: {num_test_samples}, Classes: {num_classes}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "pool_layer = GlobalAveragePooling2D()\n",
    "\n",
    "feature_extractor = Sequential([\n",
    "    base_model,\n",
    "    pool_layer\n",
    "], name=\"FeatureExtractor\")\n",
    "\n",
    "feature_extractor.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "print(\"Extracting training embeddings...\")\n",
    "X_train_embed_list = []\n",
    "y_train_list = []\n",
    "\n",
    "steps_train = len(train_generator)  # steps in one epoch for training data\n",
    "for i in range(steps_train):\n",
    "    X_batch, y_batch = train_generator[i]  # (batch_size, 224, 224, 3)\n",
    "    embeddings = feature_extractor.predict(X_batch)  # (batch_size, 2048) typically for ResNet50\n",
    "    X_train_embed_list.append(embeddings)\n",
    "    y_train_list.append(y_batch)\n",
    "\n",
    "X_train_embed = np.concatenate(X_train_embed_list, axis=0)  # shape: (num_train_samples, 2048)\n",
    "y_train = np.concatenate(y_train_list, axis=0)             # shape: (num_train_samples, num_classes)\n",
    "\n",
    "# Convert one-hot to integer labels\n",
    "y_train_int = np.argmax(y_train, axis=1)\n",
    "\n",
    "print(\"X_train_embed shape:\", X_train_embed.shape)\n",
    "print(\"y_train_int shape:\", y_train_int.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "smote = SMOTE(random_state=42)\n",
    "X_train_sm, y_train_sm = smote.fit_resample(X_train_embed, y_train_int)\n",
    "\n",
    "print(\"After SMOTE:\")\n",
    "print(\"X_train_sm shape:\", X_train_sm.shape)\n",
    "print(\"y_train_sm shape:\", y_train_sm.shape)\n",
    "\n",
    "# Convert back to one-hot\n",
    "y_train_sm_onehot = tf.keras.utils.to_categorical(y_train_sm, num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms, models\n",
    "import os\n",
    "from tqdm import tqdm  # Import tqdm for progress bar\n",
    "\n",
    "# === Constants ===\n",
    "DATA_DIR = 'images/NGD_HACK_NO_BB'\n",
    "IMAGE_SIZE = (224, 224)\n",
    "BATCH_SIZE = 32 # Sutis 3GB VRAM\n",
    "SEED = 123\n",
    "NUM_CLASSES = 26\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED) if torch.cuda.is_available() else None\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# === Data Preprocessing ===\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(IMAGE_SIZE),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# === Load and Prepare Data ===\n",
    "print(f\"Loading data from {DATA_DIR}...\")\n",
    "dataset = datasets.ImageFolder(root=DATA_DIR, transform=transform)\n",
    "print(f\"Found {len(dataset)} images in {len(dataset.classes)} classes\")\n",
    "\n",
    "# Calculate sizes for train-validation split (80-20)\n",
    "dataset_size = len(dataset)\n",
    "train_size = int(dataset_size * 0.8)\n",
    "val_size = dataset_size - train_size\n",
    "\n",
    "# Create the splits\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size], \n",
    "                                          generator=torch.Generator().manual_seed(SEED))\n",
    "\n",
    "print(f\"Training set: {train_size} images\")\n",
    "print(f\"Validation set: {val_size} images\")\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "\n",
    "# === Model Creation ===\n",
    "def create_efficientnet_model(num_classes=NUM_CLASSES):\n",
    "    print(\"Loading pretrained EfficientNet-B3...\")\n",
    "    # Load pretrained EfficientNet-B3\n",
    "    model = models.efficientnet_b3(pretrained=True)\n",
    "    \n",
    "    # Freeze all base layers\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    # Replace the classifier\n",
    "    in_features = model.classifier[1].in_features\n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Dropout(p=0.3, inplace=True),\n",
    "        nn.Linear(in_features=in_features, out_features=256),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(p=0.2),\n",
    "        nn.Linear(in_features=256, out_features=num_classes)\n",
    "    )\n",
    "    \n",
    "    print(\"Model prepared with frozen base layers and new classifier\")\n",
    "    return model\n",
    "\n",
    "# === Unfreeze for Fine-tuning ===\n",
    "def unfreeze_model(model):\n",
    "    print(\"Unfreezing model for fine-tuning...\")\n",
    "    # Unfreeze all parameters\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True\n",
    "        \n",
    "    # Freeze first 100 layers (approximate conversion from TF code)\n",
    "    frozen_count = 0\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'features.0.' in name or 'features.1.' in name or 'features.2.' in name:\n",
    "            param.requires_grad = False\n",
    "            frozen_count += 1\n",
    "    \n",
    "    print(f\"Kept {frozen_count} layers frozen for fine-tuning\")\n",
    "    return model\n",
    "\n",
    "# === Training Function with Progress Bar ===\n",
    "def train_epoch(model, loader, optimizer, criterion, device, epoch, num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    # Create progress bar\n",
    "    progress_bar = tqdm(loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\", \n",
    "                       leave=False, ncols=100)\n",
    "    \n",
    "    for inputs, targets in progress_bar:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Statistics\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        batch_correct = predicted.eq(targets).sum().item()\n",
    "        correct += batch_correct\n",
    "        \n",
    "        # Update progress bar\n",
    "        batch_acc = batch_correct / targets.size(0)\n",
    "        progress_bar.set_postfix({\n",
    "            'loss': f\"{loss.item():.4f}\",\n",
    "            'acc': f\"{batch_acc:.4f}\"\n",
    "        })\n",
    "        \n",
    "    epoch_loss = running_loss / len(loader.dataset)\n",
    "    epoch_acc = correct / total\n",
    "    \n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "# === Validation Function with Progress Bar ===\n",
    "def validate(model, loader, criterion, device, epoch, num_epochs):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    # Create progress bar\n",
    "    progress_bar = tqdm(loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Valid]\", \n",
    "                       leave=False, ncols=100)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in progress_bar:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            batch_correct = predicted.eq(targets).sum().item()\n",
    "            correct += batch_correct\n",
    "            \n",
    "            # Update progress bar\n",
    "            batch_acc = batch_correct / targets.size(0)\n",
    "            progress_bar.set_postfix({\n",
    "                'loss': f\"{loss.item():.4f}\",\n",
    "                'acc': f\"{batch_acc:.4f}\"\n",
    "            })\n",
    "    \n",
    "    val_loss = running_loss / len(loader.dataset)\n",
    "    val_acc = correct / total\n",
    "    \n",
    "    return val_loss, val_acc\n",
    "\n",
    "# === Training Workflow ===\n",
    "def train_model(train_loader, val_loader, num_epochs=10):\n",
    "    # Create model\n",
    "    model = create_efficientnet_model()\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    \n",
    "    # Phase 1: Train with frozen base\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Phase 1: Training with frozen base layers\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion, device, epoch, num_epochs)\n",
    "        val_loss, val_acc = validate(model, val_loader, criterion, device, epoch, num_epochs)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - \"\n",
    "              f\"Train loss: {train_loss:.4f}, Train acc: {train_acc:.4f} - \"\n",
    "              f\"Val loss: {val_loss:.4f}, Val acc: {val_acc:.4f}\")\n",
    "    \n",
    "    # Phase 2: Fine-tune with partially unfrozen base\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Phase 2: Fine-tuning with partially unfrozen base layers\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    model = unfreeze_model(model)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-4)  # Lower learning rate for fine-tuning\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion, device, epoch, num_epochs)\n",
    "        val_loss, val_acc = validate(model, val_loader, criterion, device, epoch, num_epochs)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - \"\n",
    "              f\"Train loss: {train_loss:.4f}, Train acc: {train_acc:.4f} - \"\n",
    "              f\"Val loss: {val_loss:.4f}, Val acc: {val_acc:.4f}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# === Run Training ===\n",
    "print(\"\\nStarting training process...\")\n",
    "trained_model = train_model(train_loader, val_loader)\n",
    "\n",
    "# Save the model\n",
    "print(\"\\nSaving model...\")\n",
    "torch.save(trained_model.state_dict(), \"ngd_efficientnet_model.pth\")\n",
    "print(\"Model saved successfully as 'ngd_efficientnet_model.pth'!\")\n",
    "\n",
    "# Print completion message\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Training complete!\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
